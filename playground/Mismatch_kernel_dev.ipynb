{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "from data_handler import *\n",
    "from kernel_methods import *\n",
    "from metrics import *\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading training data\n",
    "tr0 = load_data(0, 'tr')\n",
    "tr1 = load_data(1, 'tr')\n",
    "tr2 = load_data(2, 'tr')\n",
    "\n",
    "## Loading test data\n",
    "te0 = load_data(0, 'te')\n",
    "te1 = load_data(1, 'te')\n",
    "te2 = load_data(2, 'te')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the mismatch kernel functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(alphabet, substring_length):\n",
    "    '''\n",
    "    Create all the vocabulary of all possibles words using the alphabet: all\n",
    "    combination of length substring_length. Vocabulary is of size |alphabet|^substring_length.\n",
    "    \n",
    "    Input:\n",
    "        alphabet: letters available in the alphabet\n",
    "        substring_length: lenghth of words\n",
    "        \n",
    "    Output:\n",
    "        vocab2index: dictionary associating each word in the vocab to an index (integer)\n",
    "        index2vocab: dictionary associating each index to a word in the vocab\n",
    "    '''\n",
    "    vocab = [''.join(i) for i in itertools.product(alphabet, repeat = substring_length)]\n",
    "    \n",
    "    vocab2index = {}\n",
    "    index2vocab = {}\n",
    "    for idx, v in enumerate(vocab):\n",
    "        vocab2index[v] = idx\n",
    "        index2vocab[idx] = v\n",
    "        \n",
    "    return vocab2index, index2vocab\n",
    "\n",
    "\n",
    "def is_neighbour(alpha, beta, mismatch):\n",
    "    '''\n",
    "    Check if word beta is in the neighbourhood of word alpha as defined by Leslie and al.\n",
    "    http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.7384&rep=rep1&type=pdf\n",
    "    \n",
    "    Input:\n",
    "        alpha: first word\n",
    "        beta: second word\n",
    "        mismatch: tolerance of mismatch\n",
    "    Output\n",
    "        Boolean: True if beta is the mismatch-neighbourhood of alpha\n",
    "    '''\n",
    "    if sum(a!=b for a, b in zip(alpha, beta)) <= mismatch:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def compute_neighbours(vocab2index, mismatch):\n",
    "    '''\n",
    "    Compute once for all the neighbours of each word in the vocabulary.\n",
    "    \n",
    "    Input:\n",
    "        vocab2index: vocabulary\n",
    "        mismatch: tolerance of mismatch\n",
    "    Output:\n",
    "        Dictionary of neighbours for each word in the vocabulary.\n",
    "    '''\n",
    "    vocab = vocab2index.keys()\n",
    "    \n",
    "    neighbours = {}\n",
    "    for word1 in vocab:\n",
    "        neighbours[word1] = []\n",
    "        for word2 in vocab:\n",
    "            if is_neighbour(word1, word2, mismatch):\n",
    "                neighbours[word1].append(word2)\n",
    "    \n",
    "    return neighbours\n",
    "\n",
    "\n",
    "def create_mismatch_feature(sequence, substring_length, vocab2index, neighbours):\n",
    "    '''\n",
    "    Mismatch kernel feature as described by Leslie and al.\n",
    "    http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.7384&rep=rep1&type=pdf\n",
    "    \n",
    "    Input:\n",
    "        sequence: DNA sequence to process\n",
    "        substring_length: lenghth of vocabulary words\n",
    "        vocab2index: mapping of vocabulary word to their index\n",
    "        neighbours: neighbours for each word for each of the word in the vocabulary\n",
    "    Output:\n",
    "        Numpy array: Sequence embedding\n",
    "    '''\n",
    "    embedding = np.zeros(len(vocab2index), dtype = 'int')\n",
    "\n",
    "    for start in range(len(sequence) - substring_length + 1):\n",
    "        end = start + substring_length\n",
    "        substring = sequence[start:end]\n",
    "        for neighbour in neighbours[substring]:\n",
    "            embedding[vocab2index[neighbour]] += 1\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def mismatch_kernel(sequenceA, sequenceB, substring_length, vocab2index, neighbours, normalize = False):\n",
    "    '''\n",
    "    Mismatch kernel. Optional normalization as described in Leslie and al.\n",
    "    '''\n",
    "    embedingA = create_mismatch_feature(sequenceA, substring_length, vocab2index, neighbours)\n",
    "    embedingB = create_mismatch_feature(sequenceB, substring_length, vocab2index, neighbours)\n",
    "    \n",
    "    if normalize:\n",
    "        return np.dot(embedingA, embedingB)/(np.linalg.norm(embedingA)*np.linalg.norm(embedingB))\n",
    "    else:\n",
    "        return np.dot(embeddingA, embeddingB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alphabet\n",
    "alphabet = ['A', 'C', 'G', 'T']\n",
    "\n",
    "substring_length = 5\n",
    "mismatch_tol = 1\n",
    "\n",
    "vocab2index, _ = create_vocab(alphabet, substring_length)\n",
    "neighbours = compute_neighbours(vocab2index, mismatch_tol)\n",
    "\n",
    "\n",
    "#Example\n",
    "# create_mismatch_feature(tr0['Sequence'][10], substring_length, vocab2index, neighbours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building kernel matrix from 2000x2000 samples...\n"
     ]
    }
   ],
   "source": [
    "lbda = 0.005\n",
    "kSVM = kernelSVM(lbda)\n",
    "\n",
    "data = tr0\n",
    "\n",
    "#Train\n",
    "kSVM.train(tr0['Sequence'].as_matrix(), \n",
    "           tr0['Bound'].as_matrix(), \n",
    "           kernel_fct = lambda seq_A, seq_B: mismatch_kernel(seq_A, seq_B, substring_length, vocab2index, neighbours, normalize = True))\n",
    "\n",
    "#Test\n",
    "# te0 = load_data(0, 'te')\n",
    "# predictions = kSVM.predict(te0['Sequence'], stringsData = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_sums = np.linalg.norm(X, axis = 1)\n",
    "X = X / row_sums[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engaging n-fold cross validation with 5 folds on 2000 items\n",
      "Fold 0, Match rate: 0.71\n",
      "Fold 1, Match rate: 0.68\n",
      "Fold 2, Match rate: 0.67\n",
      "Fold 3, Match rate: 0.67\n",
      "Fold 4, Match rate: 0.72\n",
      "Done! Average Match rate is 0.69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6880000000000001"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C = 0.05)\n",
    "\n",
    "\n",
    "data = tr0\n",
    "\n",
    "X = np.zeros((len(data), len(vocab2index)))\n",
    "for idx, seq in enumerate(data['Sequence']):\n",
    "    X[idx, :] = create_mismatch_feature(seq, substring_length, vocab2index, neighbours)\n",
    "# row_sums = np.linalg.norm(X, axis = 1)\n",
    "# X = X / row_sums[:, np.newaxis]\n",
    "Y = data['Bound'].as_matrix()\n",
    "\n",
    "\n",
    "kfold(data = X, \n",
    "      labels = Y, \n",
    "      n_folds = 5,\n",
    "      train_method = clf.fit, \n",
    "      pred_method = clf.predict,\n",
    "      metric = m_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - l1 penalty - C: 0.0001 - Mean Cross Validation Score: 50.00%\n",
      "Logistic Regression - l1 penalty - C: 0.0005 - Mean Cross Validation Score: 50.00%\n",
      "Logistic Regression - l1 penalty - C: 0.001 - Mean Cross Validation Score: 50.00%\n",
      "Logistic Regression - l1 penalty - C: 0.005 - Mean Cross Validation Score: 52.85%\n",
      "Logistic Regression - l1 penalty - C: 0.01 - Mean Cross Validation Score: 65.40%\n",
      "Logistic Regression - l1 penalty - C: 0.05 - Mean Cross Validation Score: 72.10%\n",
      "Logistic Regression - l2 penalty - C: 0.0001 - Mean Cross Validation Score: 70.75%\n",
      "Logistic Regression - l2 penalty - C: 0.0005 - Mean Cross Validation Score: 73.05%\n",
      "Logistic Regression - l2 penalty - C: 0.001 - Mean Cross Validation Score: 73.35%\n",
      "Logistic Regression - l2 penalty - C: 0.005 - Mean Cross Validation Score: 72.25%\n",
      "Logistic Regression - l2 penalty - C: 0.01 - Mean Cross Validation Score: 71.25%\n",
      "Logistic Regression - l2 penalty - C: 0.05 - Mean Cross Validation Score: 68.75%\n"
     ]
    }
   ],
   "source": [
    "data = tr0\n",
    "\n",
    "X = np.zeros((len(data), len(vocab2index)))\n",
    "for idx, seq in enumerate(data['Sequence']):\n",
    "    X[idx, :] = create_mismatch_feature(seq, substring_length, vocab2index, neighbours)\n",
    "Y = data['Bound']\n",
    "\n",
    "#Logistic Regrression\n",
    "for penal in ['l1', 'l2']:\n",
    "#     for regu in [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]:\n",
    "    for regu in [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]:\n",
    "        model = LogisticRegression(penalty = penal, C = regu, random_state = 777)\n",
    "        score = np.mean(cross_val_score(model, X, Y, cv = 5))\n",
    "        print('Logistic Regression - {0} penalty - C: {1} - Mean Cross Validation Score: {2:.2f}%'.format(penal, regu, 100*score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - l1 penalty - C: 0.0001 - Mean Cross Validation Score: 50.00%\n",
      "Logistic Regression - l1 penalty - C: 0.0005 - Mean Cross Validation Score: 50.00%\n",
      "Logistic Regression - l1 penalty - C: 0.001 - Mean Cross Validation Score: 50.00%\n",
      "Logistic Regression - l1 penalty - C: 0.005 - Mean Cross Validation Score: 72.20%\n",
      "Logistic Regression - l1 penalty - C: 0.01 - Mean Cross Validation Score: 79.50%\n",
      "Logistic Regression - l1 penalty - C: 0.05 - Mean Cross Validation Score: 84.40%\n",
      "Logistic Regression - l2 penalty - C: 0.0001 - Mean Cross Validation Score: 79.50%\n",
      "Logistic Regression - l2 penalty - C: 0.0005 - Mean Cross Validation Score: 84.45%\n",
      "Logistic Regression - l2 penalty - C: 0.001 - Mean Cross Validation Score: 85.25%\n",
      "Logistic Regression - l2 penalty - C: 0.005 - Mean Cross Validation Score: 84.75%\n",
      "Logistic Regression - l2 penalty - C: 0.01 - Mean Cross Validation Score: 84.45%\n",
      "Logistic Regression - l2 penalty - C: 0.05 - Mean Cross Validation Score: 82.80%\n"
     ]
    }
   ],
   "source": [
    "data = tr1\n",
    "\n",
    "X = np.zeros((len(data), len(vocab2index)))\n",
    "for idx, seq in enumerate(data['Sequence']):\n",
    "    X[idx, :] = create_mismatch_feature(seq, substring_length, vocab2index, neighbours)\n",
    "Y = data['Bound']\n",
    "\n",
    "#Logistic Regrression\n",
    "for penal in ['l1', 'l2']:\n",
    "#     for regu in [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]:\n",
    "    for regu in [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]:\n",
    "        model = LogisticRegression(penalty = penal, C = regu, random_state = 777)\n",
    "        score = np.mean(cross_val_score(model, X, Y, cv = 5))\n",
    "        print('Logistic Regression - {0} penalty - C: {1} - Mean Cross Validation Score: {2:.2f}%'.format(penal, regu, 100*score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - l1 penalty - C: 0.0001 - Mean Cross Validation Score: 50.00%\n",
      "Logistic Regression - l1 penalty - C: 0.0005 - Mean Cross Validation Score: 50.00%\n",
      "Logistic Regression - l1 penalty - C: 0.001 - Mean Cross Validation Score: 50.00%\n",
      "Logistic Regression - l1 penalty - C: 0.005 - Mean Cross Validation Score: 54.85%\n",
      "Logistic Regression - l1 penalty - C: 0.01 - Mean Cross Validation Score: 58.25%\n",
      "Logistic Regression - l1 penalty - C: 0.05 - Mean Cross Validation Score: 63.25%\n",
      "Logistic Regression - l2 penalty - C: 0.0001 - Mean Cross Validation Score: 62.75%\n",
      "Logistic Regression - l2 penalty - C: 0.0005 - Mean Cross Validation Score: 64.30%\n",
      "Logistic Regression - l2 penalty - C: 0.001 - Mean Cross Validation Score: 64.35%\n",
      "Logistic Regression - l2 penalty - C: 0.005 - Mean Cross Validation Score: 63.55%\n",
      "Logistic Regression - l2 penalty - C: 0.01 - Mean Cross Validation Score: 63.50%\n",
      "Logistic Regression - l2 penalty - C: 0.05 - Mean Cross Validation Score: 62.85%\n"
     ]
    }
   ],
   "source": [
    "data = tr2\n",
    "\n",
    "X = np.zeros((len(data), len(vocab2index)))\n",
    "for idx, seq in enumerate(data['Sequence']):\n",
    "    X[idx, :] = create_mismatch_feature(seq, substring_length, vocab2index, neighbours)\n",
    "Y = data['Bound']\n",
    "\n",
    "#Logistic Regrression\n",
    "for penal in ['l1', 'l2']:\n",
    "#     for regu in [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]:\n",
    "    for regu in [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]:\n",
    "        model = LogisticRegression(penalty = penal, C = regu, random_state = 777)\n",
    "        score = np.mean(cross_val_score(model, X, Y, cv = 5))\n",
    "        print('Logistic Regression - {0} penalty - C: {1} - Mean Cross Validation Score: {2:.2f}%'.format(penal, regu, 100*score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
