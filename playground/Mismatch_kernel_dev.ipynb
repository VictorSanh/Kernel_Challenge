{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "from data_handler import *\n",
    "from kernel_methods import *\n",
    "from metrics import *\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Alphabet\n",
    "alphabet = ['A', 'C', 'G', 'T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading training data\n",
    "tr0 = load_data(0, 'tr')\n",
    "tr1 = load_data(1, 'tr')\n",
    "tr2 = load_data(2, 'tr')\n",
    "\n",
    "## Loading test data\n",
    "te0 = load_data(0, 'te')\n",
    "te1 = load_data(1, 'te')\n",
    "te2 = load_data(2, 'te')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the mismatch kernel functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(alphabet, substring_length):\n",
    "    '''\n",
    "    Create all the vocabulary of all possibles words using the alphabet: all\n",
    "    combination of length substring_length. Vocabulary is of size |alphabet|^substring_length.\n",
    "    \n",
    "    Input:\n",
    "        alphabet: letters available in the alphabet\n",
    "        substring_length: lenghth of words\n",
    "        \n",
    "    Output:\n",
    "        vocab2index: dictionary associating each word in the vocab to an index (integer)\n",
    "        index2vocab: dictionary associating each index to a word in the vocab\n",
    "    '''\n",
    "    vocab = [''.join(i) for i in itertools.product(alphabet, repeat = substring_length)]\n",
    "    \n",
    "    vocab2index = {}\n",
    "    index2vocab = {}\n",
    "    for idx, v in enumerate(vocab):\n",
    "        vocab2index[v] = idx\n",
    "        index2vocab[idx] = v\n",
    "        \n",
    "    return vocab2index, index2vocab\n",
    "\n",
    "\n",
    "def is_neighbour(alpha, beta, mismatch):\n",
    "    '''\n",
    "    Check if word beta is in the neighbourhood of word alpha as defined by Leslie and al.\n",
    "    http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.7384&rep=rep1&type=pdf\n",
    "    \n",
    "    Input:\n",
    "        alpha: first word\n",
    "        beta: second word\n",
    "        mismatch: tolerance of mismatch\n",
    "    Output\n",
    "        Boolean: True if beta is the mismatch-neighbourhood of alpha\n",
    "    '''\n",
    "    if sum(a!=b for a, b in zip(alpha, beta)) <= mismatch:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def compute_neighbours(vocab2index, mismatch):\n",
    "    '''\n",
    "    Compute once for all the neighbours of each word in the vocabulary.\n",
    "    \n",
    "    Input:\n",
    "        vocab2index: vocabulary\n",
    "        mismatch: tolerance of mismatch\n",
    "    Output:\n",
    "        Dictionary of neighbours for each word in the vocabulary.\n",
    "    '''\n",
    "    vocab = vocab2index.keys()\n",
    "    \n",
    "    neighbours = {}\n",
    "    for word1 in vocab:\n",
    "        neighbours[word1] = []\n",
    "        for word2 in vocab:\n",
    "            if is_neighbour(word1, word2, mismatch):\n",
    "                neighbours[word1].append(word2)\n",
    "    \n",
    "    return neighbours\n",
    "\n",
    "\n",
    "def create_mismatch_feature(sequence, substring_length, vocab2index, neighbours, normalize = False):\n",
    "    '''\n",
    "    Mismatch kernel feature as described by Leslie and al.\n",
    "    http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.7384&rep=rep1&type=pdf\n",
    "    \n",
    "    Input:\n",
    "        sequence: DNA sequence to process\n",
    "        substring_length: lenghth of vocabulary words\n",
    "        vocab2index: mapping of vocabulary word to their index\n",
    "        neighbours: neighbours for each word for each of the word in the vocabulary\n",
    "    Output:\n",
    "        Numpy array: Sequence embedding\n",
    "    '''\n",
    "    embedding = np.zeros(len(vocab2index), dtype = 'int')\n",
    "\n",
    "    for start in range(len(sequence) - substring_length + 1):\n",
    "        end = start + substring_length\n",
    "        substring = sequence[start:end]\n",
    "        for neighbour in neighbours[substring]:\n",
    "            embedding[vocab2index[neighbour]] += 1\n",
    "    \n",
    "    if normalize:\n",
    "        embedding = embedding/np.linalg.norm(embedding)\n",
    "        \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def mismatch_kernel(sequenceA, sequenceB, substring_length, vocab2index, neighbours, normalize):\n",
    "    '''\n",
    "    Mismatch kernel. Optional normalization as described in Leslie and al.\n",
    "    '''\n",
    "    embedingA = create_mismatch_feature(sequenceA, substring_length, vocab2index, neighbours, normalize)\n",
    "    embedingB = create_mismatch_feature(sequenceB, substring_length, vocab2index, neighbours, normalize)\n",
    "    \n",
    "    return np.dot(embeddingA, embeddingB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alphabet\n",
    "alphabet = ['A', 'C', 'G', 'T']\n",
    "\n",
    "substring_length = 3\n",
    "mismatch_tol = 1\n",
    "\n",
    "vocab2index, _ = create_vocab(alphabet, substring_length)\n",
    "neighbours = compute_neighbours(vocab2index, mismatch_tol)\n",
    "\n",
    "\n",
    "#Example\n",
    "# create_mismatch_feature(tr0['Sequence'][10], substring_length, vocab2index, neighbours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbda = 0.005\n",
    "kSVM = kernelSVM(lbda)\n",
    "\n",
    "data = tr0\n",
    "\n",
    "#Train\n",
    "kSVM.train(tr0['Sequence'].as_matrix(), \n",
    "           tr0['Bound'].as_matrix(), \n",
    "           kernel_fct = lambda seq_A, seq_B: mismatch_kernel(seq_A, seq_B, substring_length, vocab2index, neighbours, normalize = True))\n",
    "\n",
    "#Test\n",
    "# te0 = load_data(0, 'te')\n",
    "# predictions = kSVM.predict(te0['Sequence'], stringsData = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search With Mismatch Kernel Features + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_log_reg(data, sub_lengths, tols, regularizations, penalties, normalize):\n",
    "    max_score = 0\n",
    "    max_model = None\n",
    "\n",
    "    Y = data['Bound'].as_matrix()\n",
    "\n",
    "    for substring_length in sub_lengths:\n",
    "        vocab2index, _ = create_vocab(alphabet, substring_length)\n",
    "        for mismatch_tol in tols:\n",
    "            print('--Normalize: {0} - Substring Length: {1} - Mismatch Tolerance: {2}'.format(normalize, substring_length, mismatch_tol))\n",
    "            neighbours = compute_neighbours(vocab2index, mismatch_tol)\n",
    "\n",
    "            X = np.zeros((len(data), len(vocab2index)))\n",
    "            for idx, seq in enumerate(data['Sequence']):\n",
    "                X[idx, :] = create_mismatch_feature(seq, substring_length, vocab2index, neighbours, normalize)\n",
    "\n",
    "            for penal in penalties:\n",
    "                for regu in regularizations:\n",
    "                    clf = LogisticRegression(C = regu, penalty = penal)\n",
    "                    #SIMON. Cette partie la bug... à cause du sign_label je crois...\n",
    "#                     score = kfold(data = X, \n",
    "#                                   labels = Y, \n",
    "#                                   n_folds = 5,\n",
    "#                                   train_method = clf.fit, \n",
    "#                                   pred_method = clf.predict,\n",
    "#                                   metric = m_binary,\n",
    "#                                   verbose = True)\n",
    "                    #Du coup, j'ai utilisé celle de sklearn.\n",
    "                    score = np.mean(cross_val_score(clf, X, Y, cv = 5))\n",
    "\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "                        max_model = clf\n",
    "                        print('----Increase in Score. Penal: {0} - Regu: {1}. Mean Val Score: {2:.2f}'.format(penal, regu, 100*score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = tr0\n",
    "\n",
    "sub_lengths = [3,4,5,6]\n",
    "tols = [1, 2]\n",
    "regularizations = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]\n",
    "penalties = ['l1', 'l2']\n",
    "\n",
    "gd_log_reg(data, sub_lengths, tols, regularizations, penalties, normalize= True)\n",
    "print('\\n')\n",
    "gd_log_reg(data, sub_lengths, tols, regularizations, penalties, normalize= False)\n",
    "\n",
    "print('\\nFinished !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tr1\n",
    "\n",
    "sub_lengths = [3,4,5,6]\n",
    "tols = [1, 2]\n",
    "regularizations = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "penalties = ['l1', 'l2']\n",
    "\n",
    "gd_log_reg(data, sub_lengths, tols, regularizations, penalties, normalize= True)\n",
    "gd_log_reg(data, sub_lengths, tols, regularizations, penalties, normalize= False)\n",
    "\n",
    "print('\\nFinished !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tr2\n",
    "\n",
    "sub_lengths = [3,4,5,6]\n",
    "tols = [1, 2]\n",
    "regularizations = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "penalties = ['l1', 'l2']\n",
    "\n",
    "gd_log_reg(data, sub_lengths, tols, regularizations, penalties, normalize= True)\n",
    "gd_log_reg(data, sub_lengths, tols, regularizations, penalties, normalize= False)\n",
    "\n",
    "print('\\nFinished !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
