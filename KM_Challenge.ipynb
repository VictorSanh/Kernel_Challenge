{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods challenge\n",
    "\n",
    "Importing base libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Debugging requirements\n",
    "import pdb\n",
    "\n",
    "## Performance metrics requirements\n",
    "import time\n",
    "\n",
    "## Kernel SVM requirements\n",
    "from cvxopt import matrix\n",
    "from cvxopt import solvers\n",
    "import mosek\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from numpy.core.defchararray import not_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Loading the data + sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run data_handler.py\n",
    "\n",
    "## Loading training data\n",
    "tr0 = load_data(0, 'tr')\n",
    "tr1 = load_data(1, 'tr')\n",
    "tr2 = load_data(2, 'tr')\n",
    "\n",
    "## Loading test data\n",
    "te0 = load_data(0, 'te')\n",
    "te1 = load_data(1, 'te')\n",
    "te2 = load_data(2, 'te')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sanity checks..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training set 0</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr0['Bound'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr0.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr0.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training set 1</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr1['Bound'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr1.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training set 2</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr2['Bound'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr2.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Test set 0</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te0['Sequence'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te0.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te0.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Test set 1</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te1['Sequence'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te1.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Test set 2</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te2['Sequence'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te2.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First idea: use some distance on the strings as a kernel.\n",
    "However, note that some distances (Hamming) are only defined for sequences of the same size.\n",
    "What is the mininimum and maximum length of the DNA sequences in this first train set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_length = tr0['Sequence'].str.len().max(0)\n",
    "max_length = tr0['Sequence'].str.len().max(0)\n",
    "print('Min sequence length: {}'.format(min_length))\n",
    "print('Max sequence length: {}'.format(max_length))\n",
    "print('Length amplitude: {}'.format(max_length-min_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining first kernels + running simple classification model\n",
    "\n",
    "### First kernels\n",
    "\n",
    "Ok, so here all sequences have the same length. That means that we can start by something simple like Hamming. However, we may want to use something that would seamlessly extend to DNA sequences of different lengths...\n",
    "Here I will test both the Hamming and the Levenshtein distance as kernels for mapping DNA sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run kernels.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing kernel computation speed (debugging only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "Ktr0 = build_kernel(tr0['Sequence'], tr0['Sequence'], kernel_fct = hamming_distance)\n",
    "t1 = time.time()\n",
    "Ktr1 = build_kernel(tr1['Sequence'], tr1['Sequence'], kernel_fct = hamming_distance)\n",
    "t2 = time.time()\n",
    "Ktr2 = build_kernel(tr2['Sequence'], tr2['Sequence'], kernel_fct = hamming_distance)\n",
    "t3 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Preparing kernel matrix for a training dataset 1 took {0:d}min {1:d}s with this method'.format(int((t1-t0)/60),int(t1-t0)%60))\n",
    "print('Preparing kernel matrix for a training dataset 2 took {0:d}min {1:d}s with this method'.format(int((t2-t1)/60),int(t2-t1)%60))\n",
    "print('Preparing kernel matrix for a training dataset 3 took {0:d}min {1:d}s with this method'.format(int((t3-t2)/60),int(t3-t2)%60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "Defining a couple of losses functions that will be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run metrics.py\n",
    "        \n",
    "m_binary = Metric('Match rate', lambda preds,labels: 1 - ls_binary(preds,labels), quantized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel method parent & kernel SVM\n",
    "\n",
    "Throughout the challenge we will need to use different kernel methods, which will share some attributes and methods. I will thus create an \"abstract\" class kernelMethod, and derive a kernelSVM class from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I will try out is a kernel SVM method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run kernel_methods.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing SVM implementation with a linear SVM on iris dataset\n",
    "\n",
    "First let's test the KernelSVM class that we've built on simple data, coming from the IRIS dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_file  = 'misc_data/Iris.csv'\n",
    "iris = pd.read_csv(iris_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = iris.assign(label=(iris['Species']=='Iris-setosa'))\n",
    "_ = iris.pop('Species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbda2 = 0.01\n",
    "lSVM = kernelSVM(lbda2)\n",
    "iris_X = iris.drop(['Id','label'], axis=1).as_matrix()\n",
    "iris_X_res = iris_X[:,:2]\n",
    "iris_Y = iris['label'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's test training a linear SVM on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lSVM.train(iris_X_res, iris_Y, kernel_fct=None, stringsData=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_preds = lSVM.predict(iris_X_res, stringsData=False)\n",
    "iris_preds = np.ravel(lSVM.classify(raw_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(iris_X_res[:,0],iris_X_res[:,1],c=iris_Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(iris_X_res[:,0],iris_X_res[:,1],c=iris_preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's try a cross-validation with 5 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lSVM.grid_search(iris_X_res, iris_Y, 0.0000001, 100, 10, n_folds=5, scale='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lSVM.lbda = 0.01\n",
    "_ = lSVM.assess(iris_X_res, iris_Y, n_folds=5, stringsData=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Great! It looks like the kernelSVM class is fully functionnal on a linear kernel with vector data.\n",
    "\n",
    "## KernelSVM for predicting transcription factor binding\n",
    "Now let's try our kernelSVM with some basic kernels:\n",
    "- based on Hamming distance (acceptable in terms of computation time for our purpose)\n",
    "- based on Levenshtein distance? (would seem more relevant to the problem, however computational issues are abound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Method definition\n",
    "lbda = 0.5\n",
    "kSVM = kernelSVM(lbda)\n",
    "\n",
    "## We'll try out the spectrum kernel first\n",
    "substring_length = 3\n",
    "dict0 = create_dictionary(tr0['Sequence'], substring_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Training SVM + performance assessment on training data\n",
    "kSVM_tr0_score = kSVM.assess(tr0['Sequence'].as_matrix(), tr0['Bound'].as_matrix(), kernel_fct = lambda x,y: spectrum_kernel(x, y, substring_length, dict0), n_folds = 5, metric=m_binary)\n",
    "kSVM.train(tr0['Sequence'].as_matrix(), tr0['Bound'].as_matrix(), kernel_fct = lambda x,y: spectrum_kernel(x, y, substring_length, dict0))\n",
    "kSVM_te0_raw = kSVM.classify(kSVM.predict(te0['Sequence'].as_matrix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Training SVM + performance assessment on training data\n",
    "kSVM_tr1_score = kSVM.assess(tr1['Sequence'].as_matrix(), tr1['Bound'].as_matrix(), kernel_fct = hamming_kernel, n_folds = 5, metric=m_binary)\n",
    "kSVM.train(tr1['Sequence'].as_matrix(), tr1['Bound'].as_matrix(), hamming_kernel)\n",
    "kSVM_te1_raw = np.sign(kSVM.predict(te1['Sequence'])).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Training SVM + performance assessment on training data\n",
    "kSVM_tr2_score = kSVM.assess(tr2['Sequence'].as_matrix(), tr2['Bound'].as_matrix(), kernel_fct = hamming_kernel, n_folds = 5, metric=m_binary)\n",
    "kSVM.train(tr2['Sequence'].as_matrix(), tr2['Bound'].as_matrix(), hamming_kernel)\n",
    "kSVM_te2_raw = np.sign(kSVM.predict(te2['Sequence'])).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Next steps - Results</b>:\n",
    "- What is the reason for such a poor performance rate, even on the training data?\n",
    "- If this is due to Hamming being mostly irrelevant, implement the Levenshtein distance and retry with this new kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Next steps - Computing speed</b>:\n",
    "- Find a way to vectorize the kernel matrix computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Predictions on test data\n",
    "kSVM_te0 = pd.DataFrame(\n",
    "    data = format_preds(kSVM_te0_raw),\n",
    "    columns = ['Bound'])\n",
    "\n",
    "kSVM_te1 = pd.DataFrame(\n",
    "    data = format_preds(kSVM_te1_raw),\n",
    "    columns = ['Bound'])\n",
    "kSVM_te1.index = kSVM_te1.index + 1000\n",
    "\n",
    "kSVM_te2 = pd.DataFrame(\n",
    "    data = format_preds(kSVM_te2_raw),\n",
    "    columns = ['Bound'])\n",
    "kSVM_te2.index = kSVM_te2.index + 2000\n",
    "\n",
    "frames = [kSVM_te0, kSVM_te1, kSVM_te2]\n",
    "kSVM_te = pd.concat(frames)\n",
    "kSVM_te.index = kSVM_te.index.set_names(['Id'])\n",
    "\n",
    "kSVM_te.to_csv('predictions/kSVM_te.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Trying out kNN with Hamming (debugging only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "kNN = kernelKNN(k)\n",
    "kNN.train(tr0['Sequence'], tr0['Bound'])\n",
    "kNN.predict(te0['Sequence'], hamming_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.layers import Activation, Conv2D, Dense, Dropout, Embedding, Flatten, Input, LSTM, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model, np_utils\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "seq_len = 101\n",
    "nucl_map={'A':[1,0,0,0], 'C':[0,1,0,0], 'G':[0,0,1,0], 'T':[0,0,0,1]}\n",
    "nb_conv = 15\n",
    "\n",
    "seed = 9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Preprocessing data for NN\n",
    "\n",
    "# 1 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "def one_hot_batch(sequences, sep=\" \"):\n",
    "    oh_seqs = []\n",
    "    for seq in sequences:\n",
    "        split_seq = seq.split(sep)\n",
    "        oh_seq = [nucl_map[nucl] for nucl in split_seq]\n",
    "        oh_seqs.append(oh_seq)\n",
    "    return np.array(oh_seqs)\n",
    "\n",
    "def split_seqs(sequences, split=\" \"):\n",
    "    return [split.join(seq) for seq in sequences]\n",
    "\n",
    "tr0_split = split_seqs(tr0['Sequence'].as_matrix().tolist())\n",
    "tr0_oh = one_hot_batch(tr0_split)\n",
    "\n",
    "tr1_split = split_seqs(tr1['Sequence'].as_matrix().tolist())\n",
    "tr1_oh = one_hot_batch(tr1_split)\n",
    "\n",
    "tr2_split = split_seqs(tr2['Sequence'].as_matrix().tolist())\n",
    "tr2_oh = one_hot_batch(tr2_split)\n",
    "\n",
    "\n",
    "te0_split = split_seqs(te0['Sequence'].as_matrix().tolist())\n",
    "te0_oh = one_hot_batch(te0_split)\n",
    "\n",
    "te1_split = split_seqs(te1['Sequence'].as_matrix().tolist())\n",
    "te1_oh = one_hot_batch(te1_split)\n",
    "\n",
    "te2_split = split_seqs(te2['Sequence'].as_matrix().tolist())\n",
    "te2_oh = one_hot_batch(te2_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## One-hot encodding of labels\n",
    "tr0_oh_labels = np_utils.to_categorical(tr0['Bound'].as_matrix(), 2)\n",
    "tr1_oh_labels = np_utils.to_categorical(tr1['Bound'].as_matrix(), 2)\n",
    "tr2_oh_labels = np_utils.to_categorical(tr2['Bound'].as_matrix(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 1s 432us/step - loss: 0.2748 - acc: 0.4985\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 0s 186us/step - loss: 0.2740 - acc: 0.5075\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 0s 191us/step - loss: 0.2732 - acc: 0.5140\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 0s 179us/step - loss: 0.2725 - acc: 0.5245\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 0s 180us/step - loss: 0.2718 - acc: 0.5320\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.2712 - acc: 0.537 - 0s 181us/step - loss: 0.2711 - acc: 0.5380\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 0s 204us/step - loss: 0.2705 - acc: 0.5415\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 0s 187us/step - loss: 0.2699 - acc: 0.5465\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 0s 191us/step - loss: 0.2693 - acc: 0.5520\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 0s 196us/step - loss: 0.2686 - acc: 0.5590\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 0s 183us/step - loss: 0.2680 - acc: 0.5695\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 0s 178us/step - loss: 0.2675 - acc: 0.5660\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 0s 191us/step - loss: 0.2669 - acc: 0.5655\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 0s 176us/step - loss: 0.2663 - acc: 0.5805\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 0s 180us/step - loss: 0.2656 - acc: 0.5785\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 0s 179us/step - loss: 0.2650 - acc: 0.5910\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 0s 197us/step - loss: 0.2644 - acc: 0.5890\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 0s 190us/step - loss: 0.2638 - acc: 0.5985\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 0s 189us/step - loss: 0.2631 - acc: 0.6025\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 0s 192us/step - loss: 0.2625 - acc: 0.5975\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 0s 198us/step - loss: 0.2619 - acc: 0.6025\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 0s 189us/step - loss: 0.2612 - acc: 0.6095\n",
      "Epoch 23/200\n",
      " 864/2000 [===========>..................] - ETA: 0s - loss: 0.2604 - acc: 0.6134"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-84369b392ebe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m## Fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mshapeconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr0_oh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr0_oh_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m## Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "s_conv = 5\n",
    "\n",
    "shapeconv = Sequential()\n",
    "shapeconv.add(Conv2D(10, (s_conv,4), activation='relu',\n",
    "                input_shape=(seq_len, 4, 1)))\n",
    "shapeconv.add(Flatten())\n",
    "\n",
    "shapeconv.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1.0/np.power(10,4))))\n",
    "        \n",
    "shapeconv.add(Dense(2, activation='softmax'))\n",
    "\n",
    "## Optimization\n",
    "shapeconv.compile(loss = 'mean_squared_error',\n",
    "                 optimizer = 'SGD',\n",
    "                 metrics = ['accuracy'])\n",
    "\n",
    "## Fitting\n",
    "shapeconv.fit(tr0_oh.reshape((2000,101,4,1)), tr0_oh_labels, batch_size=32, epochs=200, verbose=1)\n",
    "    \n",
    "## Evaluation\n",
    "scores = shapeconv.evaluate(tr0_oh.reshape((2000,101,4,1)), tr0_oh_labels, verbose=1)\n",
    "\n",
    "## Plotting selected model\n",
    "# display(SVG(model_to_dot(shapeconv).create(prog='dot', format='svg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Accuracy on val: 0.5000 - Accuracy on train: 0.6981\n",
      "Test 2: Accuracy on val: 0.5030 - Accuracy on train: 0.7833\n",
      "Test 3: Accuracy on val: 0.4925 - Accuracy on train: 0.6047\n",
      "Test 4: Accuracy on val: 0.4745 - Accuracy on train: 0.6233\n",
      "Test 5: Accuracy on val: 0.5495 - Accuracy on train: 0.6287\n",
      "Test 6: Accuracy on val: 0.5225 - Accuracy on train: 0.6911\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=6, shuffle=True, random_state=seed)\n",
    "cv_scores = []\n",
    "count = 0\n",
    "regu_dense=[0.01, 0.1]\n",
    "regu_conv = [0, 0]\n",
    "\n",
    "X = tr0_oh.reshape((2000,101,4,1))\n",
    "Y = tr0_oh_labels\n",
    "\n",
    "for train, val in kfold.split(X, Y):\n",
    "\n",
    "    s1 = 1 + count%3\n",
    "    s2 = int(count/3)\n",
    "\n",
    "    shapeconv = Sequential()\n",
    "    shapeconv.add(Conv2D(10, (6,4), activation='relu',\n",
    "                input_shape=(seq_len,4, 1), kernel_regularizer=regularizers.l2(regu_conv[s2]), kernel_initializer='glorot_uniform'))\n",
    "    # shapeconv.add(MaxPooling1D(pool_size=1+count))\n",
    "    shapeconv.add(Flatten())\n",
    "\n",
    "    shapeconv.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(regu_dense[s2])))\n",
    "        \n",
    "    shapeconv.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    ## Optimization\n",
    "    shapeconv.compile(loss = 'mean_squared_error',\n",
    "                 optimizer = 'SGD',\n",
    "                 metrics = ['accuracy'])\n",
    "\n",
    "    ## Fitting\n",
    "    shapeconv.fit(X[train], Y[train], batch_size=32, epochs=50*s1, verbose=0)\n",
    "    \n",
    "    ## Evaluation\n",
    "    scores_train = shapeconv.evaluate(X[train], Y[train], verbose=0)\n",
    "    scores_val = shapeconv.evaluate(X[val], Y[val], verbose=0)\n",
    "\n",
    "    print(\"Test {0:d}: Accuracy on val: {1:.4f} - Accuracy on train: {2:.4f}\".format(1+count, scores_val[1], scores_train[1]))\n",
    "    cv_scores.append(scores[1])    \n",
    "    count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
